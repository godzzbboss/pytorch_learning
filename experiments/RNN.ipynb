{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单元版\n",
    "- nn.RNNCell\n",
    "- nn.LSTMCell\n",
    "- nn.GRUCell  \n",
    "***\n",
    "``` python\n",
    "rnn = nn.RNNCell(input_size=10, hidden_size=20, bias=True, nonlinearity='tanh')\n",
    "input = torch.randn(6, 3, 10)\n",
    "hx = torch.randn(3, 20)\n",
    "output = []\n",
    "for i in range(input.size(0)):\n",
    "    hx = rnn(input[i], hx) # 输出只有隐含状态的输出\n",
    "    output.append(hx)\n",
    "\n",
    "```\n",
    "***\n",
    "\n",
    "# 封装版\n",
    "- nn.RNN\n",
    "- nn.LSTM\n",
    "- nn.GRU \n",
    "***\n",
    "``` python\n",
    "rnn = nn.RNN(input_size=10, hidden_size=20, num_layers=2, nonlinearity='tanh', bias=True, batch_first=False, dropout=0,bidirectional=False) \n",
    "input = torch.rand(5, 3, 10) # 输入的形状为（seq_len, batch, feature） 序列长度为5， batchsize为3，特征数量为10  \n",
    "h0 = torch.randn(2, 3, 20) # h0的形状为（num_layers*num_direction, batch, hidden_size）  \n",
    "output, hn = rnn(input, h0) # output的形状为（seq_len，batch, num_directions*hidden_size）, hn的形状为（num_layers*num_directions, batch,hidden_size）  \n",
    "```\n",
    "***\n",
    "注：两者的最大区别是输入不同，单元版的输入是一个时刻的输入，而封装版的输入是一个时间序列的输入（包含序列长度）。因此如果输入的长度不一样时，可以采用单元版模块，在训练的时候使用for循环得到最终的输出；当输入长度一样时，可以采用封装版。\n",
    "\n",
    "# 词嵌入\n",
    "## 表示方法\n",
    "### one-hot表示\n",
    "独热表示的向量长度为词典的大小，向量的分量只有一个1.其他全为0,1的位置对应该词在词典中的位置。\n",
    "### 词袋模型\n",
    "one-hot是用来表示一个字或者词的，而词袋模型是用来表示一个句子或者一篇文档的，其向量长度也为字典长度，其中每个元素为字典中某个词在该句子会文档中出现的频率。  \n",
    "### TF-IDF  \n",
    "词频-逆文档频率\n",
    "$$TF_{ij}=\\frac{单词i在文档j中出现的次数}{文档j包含的总的单词数量}$$  \n",
    "$$IDF_i=log(\\frac{语料库中文档总数}{语料库中包含单词i的文档数+1})$$  \n",
    "$$TF-IDF = TF * IDF$$\n",
    "### 分布式表示word2vec\n",
    "词袋模型与TF-IDF表示方法只考虑了单词的词频，而没有考虑单词与单词之间的关系，即任意两个词之间都是独立的。为了克服这个缺点，提出了单词的分布式表示方法，其中最著名的为word2vec。  \n",
    "分布式表示的优点是解决了词汇与位置无关的问题，缺点是学习过程相对复杂且受训练语料的影响很大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM实现词性判别\n",
    "每个单词都有词性，具体表示哪种词性与单词所处的上下文相关，这种上下文关系可以使用LSTM来进行建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T13:35:46.122714Z",
     "start_time": "2020-01-16T13:35:46.106714Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义训练数据,2条\n",
    "train_data = [('The cat ate the fish'.split(), ['DET', 'NN', 'V', 'DET', 'NN']), \n",
    "              ('They read that book'.split(), ['NN', 'V', 'DET', 'NN'])]\n",
    "# 定义测试数据，1条\n",
    "test_data = [('They ate the fish'.split())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T13:35:46.869714Z",
     "start_time": "2020-01-16T13:35:46.849714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'cat': 1, 'ate': 2, 'the': 3, 'fish': 4, 'They': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "{'DET': 0, 'NN': 1, 'V': 2}\n"
     ]
    }
   ],
   "source": [
    "# 构建单词的索引字典\n",
    "word_to_idx = {}\n",
    "for sent, tags in train_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "print(word_to_idx)\n",
    "# 构建词性索引字典\n",
    "tag_to_idx = {'DET':0, 'NN':1, 'V':2}\n",
    "print(tag_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T13:39:59.105714Z",
     "start_time": "2020-01-16T13:39:59.017714Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# 构建网络，一个Embedding层，一个LSTM层，一个全连接层\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.manual_seed(100)\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # 将一个整数变成一个向量，其实就是一个全连接层，返回的词嵌入\n",
    "                                                                 # 为全连接层的weight,\n",
    "                                                                # 只不过这个全连接层的输出紧接着输入到LSTM层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \"\"\"\n",
    "            初始化cell_state和hidden_state\n",
    "        \"\"\"\n",
    "        return (torch.zeros(1, 1, self.hidden_dim).to(device),\n",
    "               torch.zeros(1, 1, self.hidden_dim).to(device))\n",
    "    def forward(self, sent):\n",
    "        \"\"\"\n",
    "            sent为句子的索引列表\n",
    "        \"\"\"\n",
    "        # 获得词嵌入矩阵\n",
    "        embeds = self.embedding(sent)\n",
    "#         print(embeds)\n",
    "#         print(self.embedding.weight)\n",
    "        # 改为Lstm的输入格式\n",
    "        embeds = embeds.view(len(sent), 1, -1)\n",
    "        lstm_out, hn = self.lstm(embeds, self.hidden) # lstm_out形状为（seq_len, batch, hidden_dim）\n",
    "        # 修改lstm_out的形状，作为全连接层的输入\n",
    "        tag_space = self.fc1(lstm_out.view(len(sent), -1)) \n",
    "#         print(tag_space.shape)\n",
    "        # 计算每个单词属于各个词性的概率\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T13:40:01.280714Z",
     "start_time": "2020-01-16T13:39:59.703714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DET': 0, 'NN': 1, 'V': 2}\n",
      "[['They', 'ate', 'the', 'fish']]\n",
      "['NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "def prepare_sent(sent, to_idx):\n",
    "    return torch.LongTensor([to_idx[i] for i in sent])\n",
    "\n",
    "EPOCH = 100\n",
    "embedding_dim = 10\n",
    "hidden_dim = 5\n",
    "\n",
    "model = LSTMTagger(embedding_dim, hidden_dim, len(word_to_idx), len(tag_to_idx))\n",
    "model = model.cuda()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    for sent, tags in train_data:\n",
    "        inputs = prepare_sent(sent, word_to_idx)\n",
    "        targets = prepare_sent(tags, tag_to_idx)\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        # 前面传播\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 优化\n",
    "        optimizer.step()\n",
    "    \n",
    "# 测试\n",
    "model.eval()\n",
    "for sent in test_data:\n",
    "    inputs = prepare_sent(sent, word_to_idx)\n",
    "    inputs = inputs.cuda()\n",
    "    outputs = model(inputs)\n",
    "#     _, pred = outputs.topk(1, dim=1)\n",
    "    _, pred = torch.max(outputs, dim=1)\n",
    "\n",
    "print(tag_to_idx)\n",
    "print(test_data)\n",
    "res = []\n",
    "for p in pred:\n",
    "    for k, v in tag_to_idx.items():\n",
    "        if p == v:\n",
    "            res.append(k)\n",
    "            break\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-16T07:08:42.864714Z",
     "start_time": "2020-01-16T07:08:42.862714Z"
    }
   },
   "source": [
    "#  LSTM预测股票行情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
