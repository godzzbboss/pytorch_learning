{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "![title](./img/Tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标量反向传播\n",
    "z = wx + b,其中w,x,b都是标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T12:39:51.804473Z",
     "start_time": "2020-01-07T12:39:51.791473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数w,b,x的梯度分别为：tensor([2.])，tensor([1.]), None\n",
      "非叶子节点y,z的梯度分别为:None,None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义输入张量x\n",
    "x = torch.Tensor([2])\n",
    "# 初始化权重参数w,偏移量b\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "# 前向传播\n",
    "y = w @ x\n",
    "z = y + b\n",
    "# 基于张量z进行梯度反向传播\n",
    "z.backward() # 当目标张量为标量时，backward()可以无需传入参数\n",
    "# 因为在执行backward()之后，计算图会自动清空（pytorch的动态图机制使得在每次前向传播时重新构建计算图）\n",
    "# 如果需要多次使用backward（）, 需设置retain_graph=True,此时梯度是累加的\n",
    "# z.backward(retain_graph=True)\n",
    "print(\"参数w,b,x的梯度分别为：{}，{}, {}\".format(w.grad, b.grad, x.grad)) # x无需求导\n",
    "print(\"非叶子节点y,z的梯度分别为:{},{}\".format(y.grad, z.grad)) #backward之后，非叶子节点的梯度自动清空"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非标量反向传播\n",
    "pytorch有个规定：不允许张量对张量求导，只允许标量对张量求导,\n",
    "因此，如果loss是张量的话，在调用backward方法时，需要传入gradient参数，\n",
    "这个参数也是一个张量，并且与loss的形状相同。\n",
    "假设loss=[l1,l2,l3]，则gradient=[g1,g2,g3]，\n",
    "在进行求导时，其实是对$loss*grandient^T$这个标量进行求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T14:09:50.496473Z",
     "start_time": "2020-01-07T14:09:50.464473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3.],\n",
      "        [2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2]], dtype=torch.float, requires_grad=True) # 定义需要求导的叶子节点,x1=1,x2=2\n",
    "y = torch.zeros(1,2)\n",
    "y[0, 0] = x[0, 0] ** 2 + 3 * x[0, 1] # y1 = x1^2 + 3 * x2\n",
    "y[0, 1] = x[0, 1] ** 2 + 2 * x[0, 0] # y2 = x2^2 + 2 * x1\n",
    "\n",
    "J = torch.zeros(2,2) # 初始化雅克比矩阵\n",
    "y.backward(gradient=torch.tensor([[1,0]], dtype=torch.float), retain_graph=True)\n",
    "J[0] = x.grad\n",
    "x.grad = torch.zeros_like(x.grad) # 当设置retain_graph为True时，梯度是累加的，因此需要清零\n",
    "y.backward(gradient=torch.tensor([[0,1]], dtype=torch.float))\n",
    "J[1] = x.grad\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码的结果是\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch神经网络工具箱\n",
    "![title](./img/nn.png)\n",
    "***\n",
    "由上图可知，构建网络层可以基于module模块中的层，例如nn.Linear或者functional模块中的层，module中的大多数层在functional中都有与之对应的函数。  \n",
    "两者的主要区别在于module中的层继承了Module类，会自动提取可学习的参数，而functional中的层更像纯函数。但是两者功能相同，性能也没多大区别。\n",
    "\n",
    "**那么在构建网络层时该如何选择呢？**   \n",
    "1. 像卷积层、全连接层、Dropout层因为含有可学习参数，所以一般使用module模块中的层;像激活函数、池化层等不含可学习参数，一般使用functional中的函数。  \n",
    "2. nn.functional.xxx需要自己定义weight，bias等参数，每次调用的时候都需要手动传入weight,bias等参数，不利于代码复用\n",
    "3. Dropout操作在训练和测试阶段是有区别的，使用nn.Xxx方式定义Dropout,在调用model.eval()之后，可以自动实现状态的转换，使用nn.functional.xxx却不行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
